import os
import argparse
import csv
import json
from typing import Dict, List, Tuple
import numpy as np

# Must match training label order
LABEL_NAMES = ["blur", "rain", "snow", "haze", "drop"]

# Must match your action naming
ACTIONS = ["A_DEBLUR", "A_DERAIN", "A_DESNOW", "A_DEHAZE", "A_DEDROP"]
ACTION_TO_IDX = {a: i for i, a in enumerate(ACTIONS)}
IDX_TO_ACTION = {i: a for i, a in enumerate(ACTIONS)}


def safe_makedirs(p: str):
    if p:
        os.makedirs(p, exist_ok=True)


def load_eval_csv(csv_path: str) -> List[Dict]:
    """
    Expected CSV columns:
      path, gt_action, pred_action, s0_blur, s0_rain, s0_snow, s0_haze, s0_drop
    """
    rows = []
    with open(csv_path, "r", newline="", encoding="utf-8") as f:
        rd = csv.DictReader(f)
        for r in rd:
            rows.append(r)
    return rows


def compute_confusion(rows: List[Dict]) -> Tuple[np.ndarray, int, int]:
    """
    Returns: confusion matrix (5x5), used_count, skipped_count
    """
    cm = np.zeros((len(ACTIONS), len(ACTIONS)), dtype=np.int64)
    used = 0
    skipped = 0

    for r in rows:
        gt = r.get("gt_action", "")
        pr = r.get("pred_action", "")
        if gt not in ACTION_TO_IDX or pr not in ACTION_TO_IDX:
            skipped += 1
            continue
        gi = ACTION_TO_IDX[gt]
        pi = ACTION_TO_IDX[pr]
        cm[gi, pi] += 1
        used += 1

    return cm, used, skipped


def per_class_accuracy(cm: np.ndarray) -> Dict[str, float]:
    out = {}
    for i in range(cm.shape[0]):
        denom = float(cm[i, :].sum())
        acc = float(cm[i, i] / denom) if denom > 0 else 0.0
        out[IDX_TO_ACTION[i]] = acc
    return out


def macro_accuracy(per_cls: Dict[str, float]) -> float:
    if not per_cls:
        return 0.0
    return float(sum(per_cls.values()) / len(per_cls))


def micro_accuracy(cm: np.ndarray) -> float:
    total = float(cm.sum())
    correct = float(np.trace(cm))
    return float(correct / total) if total > 0 else 0.0


def print_confusion(cm: np.ndarray):
    names = [IDX_TO_ACTION[i] for i in range(len(ACTIONS))]
    w = max(len(n) for n in names)
    header = "GT\\PR".ljust(w) + "  " + "  ".join([n.rjust(w) for n in names])
    print("\n[Confusion Matrix] (rows=GT, cols=Pred)")
    print(header)
    for i, gt_name in enumerate(names):
        row = cm[i]
        print(gt_name.ljust(w) + "  " + "  ".join([str(int(v)).rjust(w) for v in row]))


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="CSV generated by eval_diagnoser_folder.py")
    ap.add_argument("--out_json", default="", help="optional path to save summary json")
    ap.add_argument("--show_top_errors", type=int, default=20, help="print top-N misclassified samples")
    args = ap.parse_args()

    rows = load_eval_csv(args.csv)
    print("[CSV]", args.csv)
    print("[Rows]", len(rows))

    cm, used, skipped = compute_confusion(rows)
    print(f"[Used] {used}  [Skipped] {skipped}")

    print_confusion(cm)

    per_cls = per_class_accuracy(cm)
    mac = macro_accuracy(per_cls)
    mic = micro_accuracy(cm)

    print("\n[Per-class Acc]")
    for a in ACTIONS:
        n = int(cm[ACTION_TO_IDX[a]].sum())
        print(f"  {a}: {per_cls[a]:.4f}  (n={n})")

    print(f"\n[Macro Acc] {mac:.4f}")
    print(f"[Micro Acc] {mic:.4f}")

    # ---- top errors (optional) ----
    if args.show_top_errors > 0:
        errs = []
        for r in rows:
            gt = r.get("gt_action", "")
            pr = r.get("pred_action", "")
            if gt in ACTION_TO_IDX and pr in ACTION_TO_IDX and gt != pr:
                # also store confidence gap if available
                try:
                    s = np.array([float(r.get(f"s0_{k}", 0.0)) for k in LABEL_NAMES], dtype=np.float32)
                    pr_idx = ACTION_TO_IDX[pr]
                    gt_idx = ACTION_TO_IDX[gt]
                    margin = float(s[pr_idx] - s[gt_idx])
                except Exception:
                    margin = 0.0
                errs.append((margin, r.get("path", ""), gt, pr))

        # Sort by "most confident wrong" first
        errs.sort(key=lambda x: x[0], reverse=True)

        print(f"\n[Top-{args.show_top_errors} Errors] (most confident wrong first)")
        for i, (margin, path, gt, pr) in enumerate(errs[: args.show_top_errors], start=1):
            print(f"{i:02d}) margin={margin:+.3f}  gt={gt}  pred={pr}  path={path}")

    # ---- save json (optional) ----
    if args.out_json:
        safe_makedirs(os.path.dirname(args.out_json))
        summary = {
            "csv": args.csv,
            "rows_total": int(len(rows)),
            "rows_used": int(used),
            "rows_skipped": int(skipped),
            "confusion_matrix": cm.tolist(),
            "per_class_acc": per_cls,
            "macro_acc": float(mac),
            "micro_acc": float(mic),
            "actions": ACTIONS,
            "label_names": LABEL_NAMES,
        }
        with open(args.out_json, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2)
        print(f"\n[JSON] saved -> {args.out_json}")


if __name__ == "__main__":
    main()
